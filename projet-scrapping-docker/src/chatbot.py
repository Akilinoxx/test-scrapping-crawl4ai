#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import requests
import json
from datetime import datetime
import asyncio

# Configuration
MISTRAL_API_KEY = "aGqrF2ADcSspcpLPL5YM31lj4AUDPAce"
PINECONE_API_KEY = "pcsk_38hV8j_ArTqGXkzUQfQWxQZKM93aCrDeVseVFcnbsNEdHmDfpZgRVxVQUJ72PEHLufXwv2"
PINECONE_HOST = "https://scraped-content-wewx8nz.svc.aped-4627-b74a.pinecone.io"

class ChatBot:
    def __init__(self, mistral_api_key=None, pinecone_api_key=None, pinecone_host=None):
        """
        Initialise le chatbot avec les connexions aux APIs
        
        Args:
            mistral_api_key (str): Cl√© API Mistral
            pinecone_api_key (str): Cl√© API Pinecone  
            pinecone_host (str): Host Pinecone
        """
        self.mistral_api_key = mistral_api_key or MISTRAL_API_KEY
        self.pinecone_api_key = pinecone_api_key or PINECONE_API_KEY
        self.pinecone_host = pinecone_host or PINECONE_HOST
        
        # Configuration des headers pour les APIs
        self.mistral_headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Authorization": f"Bearer {self.mistral_api_key}"
        }
        
        self.pinecone_headers = {
            "Api-Key": self.pinecone_api_key,
            "Content-Type": "application/json"
        }
        
        # Historique de conversation
        self.conversation_history = []
        
    def search_knowledge_base(self, query, top_k=3):
        """
        Recherche dans la base de connaissances Pinecone
        
        Args:
            query (str): Requ√™te de recherche
            top_k (int): Nombre de r√©sultats √† retourner
            
        Returns:
            list: R√©sultats de la recherche avec m√©tadonn√©es
        """
        print(f"üîç Recherche dans la base de connaissances: '{query}'")
        
        try:
            # G√©n√©rer l'embedding de la requ√™te avec Mistral
            payload = {
                "model": "mistral-embed",
                "input": [query],
                "output_dtype": "float"
            }
            
            response = requests.post(
                "https://api.mistral.ai/v1/embeddings",
                headers=self.mistral_headers,
                json=payload
            )
            
            if response.status_code != 200:
                print(f"‚ùå Erreur lors de la g√©n√©ration de l'embedding: {response.text}")
                return []
            
            response_data = response.json()
            query_embedding = response_data.get('data', [])[0].get('embedding')
            
            # Rechercher dans Pinecone
            query_url = f"{self.pinecone_host}/query"
            payload = {
                "vector": query_embedding,
                "topK": top_k,
                "includeMetadata": True,
                "namespace": ""
            }
            
            response = requests.post(
                query_url,
                headers=self.pinecone_headers,
                json=payload
            )
            
            if response.status_code != 200:
                print(f"‚ùå Erreur lors de la recherche Pinecone: {response.text}")
                return []
            
            results = response.json()
            matches = results.get('matches', [])
            
            print(f"‚úÖ Trouv√© {len(matches)} r√©sultats pertinents")
            return matches
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche: {str(e)}")
            return []
    
    def generate_response(self, user_query, context_chunks):
        """
        G√©n√®re une r√©ponse en utilisant Mistral avec le contexte trouv√©
        
        Args:
            user_query (str): Question de l'utilisateur
            context_chunks (list): Chunks de contexte trouv√©s dans Pinecone
            
        Returns:
            str: R√©ponse g√©n√©r√©e
        """
        try:
            # Construire le contexte √† partir des chunks trouv√©s
            context = ""
            sources = set()
            
            for i, chunk in enumerate(context_chunks):
                metadata = chunk.get('metadata', {})
                text = metadata.get('text', '')
                source = metadata.get('source', 'Source inconnue')
                section = metadata.get('section', '')
                
                sources.add(source)
                context += f"[Extrait {i+1} - {source}]\n{text}\n\n"
            
            # Construire le prompt pour Mistral
            system_prompt = """Tu es un assistant IA sp√©cialis√© dans l'analyse de contenu web scrap√©. 
Tu r√©ponds aux questions en te basant uniquement sur le contexte fourni.
Si l'information n'est pas dans le contexte, dis-le clairement.
Sois pr√©cis et cite tes sources quand possible."""
            
            user_prompt = f"""Contexte disponible:
{context}

Question de l'utilisateur: {user_query}

R√©ponds √† la question en te basant sur le contexte fourni. Si l'information n'est pas disponible dans le contexte, dis-le clairement."""
            
            # Appel √† l'API Mistral pour la g√©n√©ration de texte
            payload = {
                "model": "mistral-small-latest",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 1000
            }
            
            response = requests.post(
                "https://api.mistral.ai/v1/chat/completions",
                headers=self.mistral_headers,
                json=payload
            )
            
            if response.status_code != 200:
                print(f"‚ùå Erreur lors de la g√©n√©ration de r√©ponse: {response.text}")
                return "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse. Erreur de l'API."
            
            response_data = response.json()
            generated_response = response_data['choices'][0]['message']['content']
            
            # Ajouter les sources √† la r√©ponse
            if sources:
                sources_text = "\n\nüìö **Sources consult√©es:**\n" + "\n".join([f"- {source}" for source in sources])
                generated_response += sources_text
            
            return generated_response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}")
            return "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse."
    
    def chat(self, user_input):
        """
        Traite une question de l'utilisateur et retourne une r√©ponse
        
        Args:
            user_input (str): Question de l'utilisateur
            
        Returns:
            str: R√©ponse du chatbot
        """
        # Rechercher dans la base de connaissances
        relevant_chunks = self.search_knowledge_base(user_input, top_k=3)
        
        if not relevant_chunks:
            return "‚ùå D√©sol√©, je n'ai trouv√© aucune information pertinente dans ma base de connaissances pour r√©pondre √† votre question."
        
        # G√©n√©rer une r√©ponse bas√©e sur le contexte trouv√©
        response = self.generate_response(user_input, relevant_chunks)
        
        # Sauvegarder dans l'historique
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "user_query": user_input,
            "response": response,
            "sources_count": len(relevant_chunks)
        })
        
        return response
    
    def show_stats(self):
        """Affiche des statistiques sur les conversations"""
        print(f"\nüìä **Statistiques de conversation:**")
        print(f"- Nombre de questions pos√©es: {len(self.conversation_history)}")
        if self.conversation_history:
            print(f"- Premi√®re question: {self.conversation_history[0]['timestamp']}")
            print(f"- Derni√®re question: {self.conversation_history[-1]['timestamp']}")
    
    def save_conversation(self, filename=None):
        """Sauvegarde l'historique de conversation"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            print(f"üíæ Conversation sauvegard√©e dans {filename}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde: {str(e)}")

def main():
    """Fonction principale pour lancer le chatbot interactif"""
    print("ü§ñ **Chatbot de Recherche S√©mantique**")
    print("=" * 50)
    print("Connect√© √† votre base vectorielle Pinecone")
    print("Tapez 'quit', 'exit' ou 'bye' pour quitter")
    print("Tapez 'stats' pour voir les statistiques")
    print("Tapez 'save' pour sauvegarder la conversation")
    print("=" * 50)
    
    # Initialiser le chatbot
    chatbot = ChatBot()
    
    # Boucle de conversation
    while True:
        try:
            # Demander une question √† l'utilisateur
            user_input = input("\n‚ùì **Votre question:** ").strip()
            
            # Commandes sp√©ciales
            if user_input.lower() in ['quit', 'exit', 'bye', 'q']:
                print("\nüëã Au revoir ! Merci d'avoir utilis√© le chatbot.")
                break
            elif user_input.lower() == 'stats':
                chatbot.show_stats()
                continue
            elif user_input.lower() == 'save':
                chatbot.save_conversation()
                continue
            elif not user_input:
                print("‚ö†Ô∏è Veuillez poser une question.")
                continue
            
            # Traiter la question
            print("\nü§î R√©flexion en cours...")
            response = chatbot.chat(user_input)
            
            # Afficher la r√©ponse
            print(f"\nü§ñ **R√©ponse:**\n{response}")
            
        except KeyboardInterrupt:
            print("\n\nüëã Au revoir ! Merci d'avoir utilis√© le chatbot.")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur inattendue: {str(e)}")

if __name__ == "__main__":
    main()
